We have several directions that we would have liked to explored given the time. 

\subsection{Improved Point Matching}
Our current point matching algorithm, while reasonably accurate, could still use some improvements. One potential idea is to incorporate multiple sources for the interesting points, such as using raw edgels. In addition, we would also like to explore using features in addition to RGB color distance for comparing regions. One such feature might be to compare the partial derivatives of the image, essentially matching the edges in the two windows. 

One suggestion that we received during our presentation was the possibility of using motion algorithms to help determine corresponding points between the two images. If we know the amount of motion that a particular object takes, we can search a small region around where we would expect the object to be in the other image to determine corresponding points. Using this in conjunction with our corner-based interesting point detection would allow us to map significantly more points, resulting in more depth data for an image. 

Finally, a way to significantly improve our point matching would be to gather intrinsic camera parameters, either by an explicit camera calibration step or using the \textit{C2MODEL} algorithm suggested in the SRI paper, and to find the Epipolar line in the corresponding image. With the Epipolar line, we would know that the corresponding point would lie somewhere on that line. This would reduce our search from a 2-dimensional search to a simple 1-dimensional search.
