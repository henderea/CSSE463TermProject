Our process for stereoscopic depth mapping consists of two components: automatically matching corresponding points in both images and then using the distance between those points to determine relative distance from the cameras. 

\subsection{Point Matching}
To be able to use triangulation, we need pairs of matched points between the two stereoscopic images. To find these matched points, we first need to find ``interesting'' points in each of the images and then try to match those points to their corresponding point in the other image. 

In theory, one constraint on the corresponding points is that they must be in the same row -- however, since this is not the perfect world, we must assume that the cameras are not necessarily perfectly lined up.Therefore, we decided that a 10 pixel horizontal search band on either side of the pixel was sufficient to cover any variations in the camera positioning while still limiting the search field quite significantly. 

To find interesting points, we tried two different algorithms: directed variance and corner detection

\subsubsection{Directed Variance}
The idea behind directed variance point finding is that easily identifiable points are those with the most variance around them. For instance, a fingerprint is much easier to track than a simple circle, where properties such as rotation cannot be readily observed. In our implementation of this algorithm, four variance maps are calculated for an image, measuring variance in a 5-pixel neighborhood along the horizontal, vertical, and diagonal axes. These four maps are then combined into one by taking the minimum of the directional variances at each pixel.

On this new combined variance map, pixels with high scores represent pixels with at least that amount or more variance in all other directions, and thus signify candidate points for our point finder. To convert this map to individual points, we iteratively maximize a threshold on this variance map, until it no longer contains a predetermined minimum number of candidate points.

While a sufficient implementation of the basic algorithm, the points selected at this stage are not quite satisfactory, and additional optimizations are needed. Specifically we noticed clusters of individual pixels within small neighborhoods of each other, and single pixels of noise being selected as candidate points. The former creates issues in point matching, as matching multiple points in too small a neighborhood becomes wasted effort, and the latter produces points which are actually not very ``interesting'', as they easily produce false matches.

Thus we encapsulate points of interest instead as regions of interest, using a simple 8-neighborhood to cluster similar pixels, and remove noise by applying a 5-pixel gaussian blur to images before processing them in our point finder.

With these two modifications, we managed to solve these two problems, however the candidate points selected by the algorithm still lacked enough of the qualities we were looking for in our points of interest. The largest issue we still faced was the point finder not finding an even enough distribution of candidate points, with it often favoring small regions of the overall image and leaving others without any points.

\subsubsection{Corner Detection}
After realizing that directed variance was not going to yield the type of interesting points that we needed, we began to look for other options for finding points that have enough unique features to them that we can accurately match them to their corresponding points in the other image. 

We ended choosing to use corner detection for finding our points of interest. Corners are fairly unique in terms of features because they have variance in both directions, meaning that they should be relatively easy to match up to other points. 

Our final point matching algorithm finds a fixed number of corners in both images and then tries to correspond the corners in each image together. The primary logic behind only matching corners to corners is that we can assume that a majority of the corners found in one image will have corresponding corners found in the other image due to the fact that both images are assumed to be taken on identical cameras at the same time while pointed at the same scene.

\subsubsection{Corresponding Areas Around Points}
To check whether a set of candidate points actually correspond, we take the sum of squared differences of the RGB color distances between the two images for a given ``window'' around each of the candidate points, trying to find the set of candidate points with the smallest difference of color distances, weighted against the distance of the candidate point away from point of interest. We perform this search for all points that are within 20 rows of the point of interest and take the point with the best correspondence. 

\subsection{Determining Relative Distances}